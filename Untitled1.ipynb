{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7KoNsHqN0VslUbNBRki7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitishRaghav/Car-price-Prediction/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1I0aa7wy5iu",
        "outputId": "f0b2aefa-cfaf-4a27-dfcd-be91a9e40016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pix2code'...\n",
            "remote: Enumerating objects: 3442, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 3442 (delta 0), reused 3 (delta 0), pack-reused 3436\u001b[K\n",
            "Receiving objects: 100% (3442/3442), 296.01 MiB | 18.63 MiB/s, done.\n",
            "Resolving deltas: 100% (1685/1685), done.\n",
            "Updating files: 100% (3486/3486), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NitishRaghav/pix2code.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, RepeatVector, concatenate,GRU, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "yS8jAi-VzO09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "def load_data(data_dir):\n",
        "    text = []\n",
        "    images=[]\n",
        "    all_filenames = listdir(data_dir)\n",
        "    all_filenames.sort()\n",
        "    images = []\n",
        "    texts = []\n",
        "    for filename in (all_filenames):\n",
        "        if str(filename).endswith(\"png\"):\n",
        "            # Load the images already prepared in arrays\n",
        "            image = Image.open(data_dir+\"/\"+filename)\n",
        "            image = image.resize((256,256)).convert('RGB')\n",
        "            images.append(np.array(image).astype('float16')/255.0)\n",
        "        elif str(filename).endswith(\".gui\"):\n",
        "            syntax = '<START> ' + load_doc(data_dir+\"/\"+filename) + ' <END>'\n",
        "            # Separate all the words with a single space\n",
        "            syntax = ' '.join(syntax.split())\n",
        "            # Add a space after each comma\n",
        "            syntax = syntax.replace(',', ' ,')\n",
        "            texts.append(syntax)\n",
        "        else:\n",
        "            print(f\"File Ignored: {filename}\")\n",
        "    images = np.array(images, dtype=float)\n",
        "    return images, texts"
      ],
      "metadata": {
        "id": "MBtix6zVzf6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features,text = load_data('/content/pix2code/all_data/')"
      ],
      "metadata": {
        "id": "8rkFh4LKzod0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape,len(text)"
      ],
      "metadata": {
        "id": "-sTqC-gZzzo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab =\", { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single\"\n",
        "Tokenizr= Tokenizer(filters='', split=\" \", lower=False)\n",
        "Tokenizr.fit_on_texts([vocab])\n",
        "vocab_size = len(Tokenizr.word_index) + 1\n"
      ],
      "metadata": {
        "id": "h85wrhzmz9Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = Tokenizr.texts_to_sequences(sequences)\n",
        "max_sequence = max(len(s) for s in train_sequences)\n",
        "\n",
        "max_length = 48\n",
        "# why max length is 48, but max_sequence is 117?\n",
        "# max_sequence is the maximum length of the sequence in the dataset, while max_length is the length of the sequence that we want to generate.\n",
        "# so basically we are lower the sequences size ?\n",
        "# yes, we are lowering the sequence size to the length of the longest sequence in the dataset.\n",
        "# why do we need to lower the sequence size?\n",
        "# The reason we are lowering the sequence size is because the model can't handle sequences of arbitrary length.\n",
        "# okay Copilot ,cool\n",
        "\n",
        "X, y, image_data = list(), list(), list()\n",
        "for img_no, seq in enumerate(train_sequences):\n",
        "    for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        X.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "        image_data.append(train_features[img_no])\n"
      ],
      "metadata": {
        "id": "UnpmMTOV0N80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, image_train, image_test = train_test_split(X, y, image_data, test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "28qo5Hge0RKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define your model architecture\n",
        "image_model = Sequential()\n",
        "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
        "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
        "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "image_model.add(Flatten())\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "image_model.add(Dense(1024, activation='relu'))\n",
        "image_model.add(Dropout(0.3))\n",
        "\n",
        "image_model.add(RepeatVector(max_length))\n",
        "\n",
        "visual_input = Input(shape=(256, 256, 3,))\n",
        "encoded_image = image_model(visual_input)\n",
        "\n",
        "language_input = Input(shape=(max_length,))\n",
        "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
        "language_model = GRU(128, return_sequences=True)(language_model)\n",
        "language_model = GRU(128, return_sequences=True)(language_model)\n",
        "\n",
        "decoder = concatenate([encoded_image, language_model])\n",
        "decoder = GRU(512, return_sequences=True)(decoder)\n",
        "decoder = GRU(512, return_sequences=False)(decoder)\n",
        "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
        "optimizer = RMSprop(learning_rate=0.0001, clipvalue=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "1RPQIiD00aat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"/weights/org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, save_freq=2)\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "metadata": {
        "id": "BP6feVHR0dOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([image_data, X], y, batch_size=35, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=50)\n",
        "# Save the final model\n",
        "model.save('models/org_model.h5')"
      ],
      "metadata": {
        "id": "1_MK4dBh0fbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}